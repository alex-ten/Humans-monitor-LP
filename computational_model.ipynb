{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T23:48:06.783220Z",
     "start_time": "2020-11-17T23:48:06.774918Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from IPython.display import display\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import numdifftools as nd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols, logit\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## Cost function\n",
    "Here we define the negative log-likelihood function of the provided parameters, given choice data. The `params` argument contains parameters the likelihood of which is evaluated. The last parameter must be the softmax temperature ($\\tau$) parameter. In `args` we provide the choice data. It consists of $K$ 2-dimensional arrays, where $K$ is equal to the number of parameters. Each array is $N \\times M$, where $N$ is the number of trials (in our case, 250) and $M$ is the number of activities (in our case, 4). Importantly, like in the `params` argument, the last element of the `args` list has a special meaning. It is reserved for the choice data, encoded as a 1-hot vector for each trial. Thus, in each row of the choice data array, all entries are 0, except for one that indicates which task was chosen on that trial. The other arrays in `args` contain learning heuristics data for each trial and each activity. The order of heuristics arrays in `args` corresponds to the order of the associated parameters (i.e. heuristic weights) in `params`.\n",
    "\n",
    "The function computes the utility of each activity on each trial ($U_{i,t}$) as a linear combination of learning heuristics ($x_i$) and their respective weights $w_i$:\n",
    "\n",
    "$$ U_{i,t} = \\sum_{i=1}^{M} w_{i} \\text{x}_{i,t} $$\n",
    "\n",
    "These utility scores are used to compute the choice probabilities for each activity:\n",
    "\n",
    "$$ p_t(\\text{choice}_i) = \\frac{e^{U_{i,t}/\\tau}}{\\sum_{i=1}^{M}e^{U_{m,t}/\\tau}} $$\n",
    "\n",
    "This gives us a 2-dimensional array, where each of N rows corresponds to choice probabilities across all M activities. The likelihood of parameters that predict choosing some activity $i$ on trial $t$ is equal to probability of choosing that activity on that trial given by the softmax model. Thus, we can get individual trial likelihoods by boolean-indexing the array of choice probabilities by the actual choice data. Finally, we can log-transform the resulting vector of likelihoods, and sum it up to get the overall log-likelihood of the model. The function returns the negative of log-likelihood, so that the optimization algorithm look for parameter values that minimize this objective. Minimizing the negative log-likelihood is equivalent to maximizing the log-likelihood, and since logarithmic transformation is monotonic, the optimal paramter values will define the maximum-likelihood model.\n",
    "\n",
    "## Model fitting\n",
    "We define `fit` and `fit_n` methods in the `SoftmaxChoiceModel` object that will help us find optimal parameter values. We use the BFGS optimization method provided by the `scipy` library. Each individual fitting routine (`SoftmaxChoiceModel.fit()`), takes in the subject's actual choice data, his or her learning heuristics data, and an initial parameter values (i.e. `initial_guess`), performs parameter optimization and returns the optimal parameters and the resulting cost value upon convergence. Since, in high-dimensional spaces the method may be prone to local optima, it is good to repeat this optimization routine several times using different initializations. This is done by `SoftmaxChoiceModel.fit_n()` that executes the fitting routing `n` times, updates the parameters found by previous routines, if they are better. Random parameter initializations are sampled uniformly from ranges provided in the `init_dict` and by the `tau_range` upon class instantiation. In the article, we report the best-fitting parameters found in `n = 300` optimization bouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T13:14:27.393871Z",
     "start_time": "2020-11-18T13:14:27.363890Z"
    },
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood(params, *args):\n",
    "    coeffs = np.array(params[:-1])\n",
    "    inps = np.stack(args[:-1], axis=0).astype(float)\n",
    "    U = (coeffs[:, None, None] * inps).sum(axis=0)\n",
    "    exponent = np.exp(U * params[-1])\n",
    "    P = (exponent.T / np.sum(exponent, axis=1)).T\n",
    "    logP = np.log(P[args[-1].astype(bool)])\n",
    "    logL = np.sum(logP, axis=0)\n",
    "    return -logL\n",
    "\n",
    "\n",
    "class SoftmaxChoiceModel(object):\n",
    "    def __init__(self, objective, data, init_dict, tau_range=[0, 10]):\n",
    "        self.objective = objective\n",
    "        self.components = init_dict.keys()\n",
    "        self.init_ranges = list(init_dict.values())\n",
    "        self.tau_range = tau_range\n",
    "        self.data = [data.loc[:, c+'1':c+'4'].values for c in self.components]\n",
    "        self.choice_data = data.loc[:, 'ch1':'ch4'].values\n",
    "        self.params = None\n",
    "        self.negloglik = None\n",
    "      \n",
    "    def fit(self, data, init_guess):           \n",
    "        results = optimize.minimize(self.objective, \n",
    "                                    x0 = init_guess, \n",
    "                                    args = data)\n",
    "        return results.x, results.fun\n",
    "\n",
    "    def fit_n(self, n, progbar=False):\n",
    "        data = tuple(self.data+[self.choice_data])\n",
    "        if progbar: \n",
    "            iter_ = tqdm(range(n), desc='Seed', leave=False)\n",
    "        else:\n",
    "            iter_ = range(n)\n",
    "        for i in iter_:\n",
    "            init_guess = np.array(\n",
    "                [np.random.uniform(l, u) for l, u in self.init_ranges+[self.tau_range]]\n",
    "            )\n",
    "            x, f = self.fit(data, init_guess)\n",
    "            if self.negloglik is None:\n",
    "                self.negloglik = f\n",
    "                self.params = x\n",
    "            else:\n",
    "                if f < self.negloglik:\n",
    "                    self.negloglik = f\n",
    "                    self.params = x\n",
    "    \n",
    "    def get_aic(self):\n",
    "        # We add 1 to the number of params to include tau\n",
    "        return 2*self.negloglik + 2*(len(self.params) + 1)\n",
    "    \n",
    "    def get_param_csv(self):\n",
    "        return ','.join(['{:.5f}'.format(p) for p in self.params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting models for comparison\n",
    "Here we fit choice data from individual subjects. For model comparisons, we derive all subsets of parameter combinations provided in the `init_dict` and perform multiple optimization bouts for each subject and each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:50:36.944634Z",
     "start_time": "2020-11-18T13:15:01.615314Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(model_data_path, nam_data_path, nruns, save_path=''):\n",
    "    df = pd.read_csv(model_data_path, index_col='sid')\n",
    "    df = df.loc[df.trial.le(60+250) & df.trial.gt(60), :]\n",
    "    df = df.loc[df.nam.gt(0), :]\n",
    "    \n",
    "    init_tau_range = [0, 10]\n",
    "    init_dict = {\n",
    "        'rpc':[-1, 1],\n",
    "        'rlp':[-1, 1],\n",
    "        'relt': [-1, 1]\n",
    "    }\n",
    "    \n",
    "    # Set up model comparison (get paramter combinations)\n",
    "    np.random.seed(1)\n",
    "    var_set = list(init_dict.keys())\n",
    "    subsets = []\n",
    "    for nb_vars in range(1, len(var_set)+1):\n",
    "        for subset in combinations(var_set, nb_vars):\n",
    "            subsets.append(subset)\n",
    "            \n",
    "    # Collect model data\n",
    "    print('Each model subset\\'s results are appended to {}'.format(path.abspath(save_path)))\n",
    "    first = True\n",
    "    for subset in tqdm(subsets, desc='Progress'):\n",
    "        comp_data = []\n",
    "        model_form = ','.join(subset)\n",
    "        init_dict_subset = {k: init_dict[k] for k in subset}\n",
    "        for i, sdf in tqdm_notebook(df.groupby('sid'), desc='Variable set = ({})'.format(model_form), leave=False):\n",
    "            model = SoftmaxChoiceModel(\n",
    "                objective = neg_log_likelihood, \n",
    "                data = sdf,\n",
    "                init_dict = init_dict_subset,\n",
    "                tau_range = init_tau_range\n",
    "            )\n",
    "            model.fit_n(nruns, progbar=True)\n",
    "            group, nam = sdf.iloc[0].loc[['group', 'nam']]\n",
    "            comp_data.append([i, group, nam, model_form, model.get_aic(), model.get_param_csv()])\n",
    "        pd.DataFrame(\n",
    "            comp_data, \n",
    "            columns=['sid', 'group', 'nam', 'vars', 'aic', 'params']\n",
    "        ).to_csv(save_path, mode='w' if first else 'a', header=first, index=False)\n",
    "        first = False\n",
    "\n",
    "    \n",
    "main(\n",
    "    model_data_path = 'data/model_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    nruns = 5,\n",
    "    save_path = 'data/model_results/param_fits.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate choices with fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T22:35:48.109718Z",
     "start_time": "2020-10-23T22:35:43.687013Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal choice model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

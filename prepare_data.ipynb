{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T20:05:27.962762Z",
     "start_time": "2020-11-18T20:05:27.913970Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from python_scripts.utils import loc_utils as lut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Combine raw data files\n",
    "Concatenate raw data and codify participant IDs into a more readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:27:53.834181Z",
     "start_time": "2020-11-16T18:27:51.838485Z"
    },
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def combine_main_raw(input_data_paths, save_path):\n",
    "    combine_list = []\n",
    "    for input_data_path in input_data_paths:\n",
    "        df = pd.read_csv(input_data_path)\n",
    "        \n",
    "        # Codify subject IDs\n",
    "        df.loc[:, 'sid'] = df.sid.astype('category').cat.codes\n",
    "        \n",
    "        # If not the 1st DF, continue enumerating from previous DF's last index\n",
    "        if combine_list:\n",
    "            last_index = combine_list[-1].loc[:, 'sid'].max()\n",
    "            df.loc[:, 'sid'] += last_index + 1\n",
    "        \n",
    "        combine_list.append(df)\n",
    "        \n",
    "    # Open main files and combine them\n",
    "    df = pd.concat(combine_list)\n",
    "\n",
    "    # Save combined data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(path.join(save_path), index=False)\n",
    "    \n",
    "    \n",
    "combine_main_raw(\n",
    "    input_data_paths = ('data/raw/ig_main.csv', 'data/raw/eg_main.csv'),\n",
    "    save_path = 'data/combined_main.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T14:17:09.510337Z",
     "start_time": "2020-10-25T14:17:09.505646Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Exclude outliers\n",
    "Exclude outliers based on allocation bias and response bias. Report number of exclusions in each group based on allocation bias, then exclude from remaining data according response bias and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:28:11.179450Z",
     "start_time": "2020-11-16T18:28:04.805775Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_clean_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Define a response bias function\n",
    "    def rbf(x):\n",
    "        _, response_counts = np.unique(x.response, return_counts=True)\n",
    "        return np.max(response_counts) / np.sum(response_counts)\n",
    "\n",
    "\n",
    "    # Open combined data file\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index('sid')\n",
    "\n",
    "    # Initialize columns to record values of interest\n",
    "    df['alloc_bias'], df['resp_bias'] = 0, 0\n",
    "\n",
    "    # Calculate values of interest\n",
    "    activities = ('A1', 'A2', 'A3', 'A4')\n",
    "    for sid, sdf in tqdm(df.groupby(by='sid'), desc='Progress: '):\n",
    "        # Allocation variance\n",
    "        counts = [sum(sdf.activity == i) for i in activities]\n",
    "        allocation_variance = np.std(counts)\n",
    "        df.loc[sid, 'alloc_bias'] = allocation_variance\n",
    "\n",
    "        # Response bias\n",
    "        response_bias = sdf.groupby('family').apply(rbf).mean()\n",
    "        df.loc[sid, 'resp_bias'] = response_bias\n",
    "\n",
    "    # Detect high allocation variance and response bias\n",
    "    df_ = df.reset_index().groupby('sid').head(1).reset_index()\n",
    "    df_['high_ab'] = df_.alloc_bias >= kwargs['ab_crit']\n",
    "    df_['high_rb'] = np.logical_and(df_.resp_bias > df_.resp_bias.mean() + kwargs['rb_crit'] * df_.resp_bias.std(), ~df_.high_ab)\n",
    "\n",
    "    display(df_.groupby(by='group')[['high_ab', 'high_rb']].sum().astype(int))\n",
    "    print('Found {} outliers'.format(np.logical_or(df_.high_ab, df_.high_rb).sum()))\n",
    "\n",
    "    # Exclude outliers\n",
    "    outlier = df_.loc[df_.high_ab | df_.high_rb, 'sid']\n",
    "    df = df.loc[~df.index.isin(outlier), :]\n",
    "    display(df.reset_index().groupby(by='group')['sid'].nunique())\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.reset_index().to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "make_clean_dataset(\n",
    "    input_data_path = 'data/combined_main.csv',\n",
    "    save_path = 'data/clean_data.csv',\n",
    "\n",
    "    # Set outlier criteria\n",
    "    ab_crit = 100,   # allocation variance critical value\n",
    "    rb_crit = 2 ,    # response bias critical value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Calculate heuristics\n",
    "|Heuristic|Description ($t_i$ = trial number $i$; $w$ = window size)|\n",
    "|:-------:|:--------------------------------------------------------|\n",
    "| **PC**  | overall competence ($t_0$ to $t_i$)                     |\n",
    "| **rPC** | recent competence ($t_{i-w}$ to $t_i$)                  |\n",
    "| **rLP** | recent learning progress ($t_{i-w}$ to $t_i$)           |\n",
    "| **SC**  | self-challenge                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:30:56.648143Z",
     "start_time": "2020-11-16T18:28:48.716635Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rlp_func(x, subwindow_1, subwindow_2, abs_lp=True):\n",
    "    '''Computing recent LP in x'''\n",
    "    diff = np.mean(x[:subwindow_1]) - np.mean(x[-subwindow_2:])\n",
    "    return np.abs(diff) if abs_lp else diff\n",
    "\n",
    "\n",
    "def make_heuristics_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Read clean data and drop unused data\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index(['sid','activity'])\n",
    "    df = df.loc[:, 'group,stage,trial,correct'.split(',')]\n",
    "    df = df.loc[df.trial <= 60+250]\n",
    "\n",
    "    # Add new columns\n",
    "    activities = 'A1,A2,A3,A4'.split(',')\n",
    "    for heuristic in ['pc','rpc','rlp']:\n",
    "        for a in activities:\n",
    "            df['{}{}'.format(heuristic, a[1])] = np.nan\n",
    "    df['sc'] = np.nan\n",
    "\n",
    "    # Calculate dynamic performance heuristics for each subject\n",
    "    act_codes = {'A1':1, 'A2':2, 'A3':3, 'A4':4}\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        for a in activities:\n",
    "            x = sdf.loc[(i, a), 'correct'].astype(int)\n",
    "\n",
    "            # Overall competence (pc)\n",
    "            pc = np.cumsum(x) / np.arange(1, x.size+1)\n",
    "            df.loc[(i, a), 'pc{}'.format(a[1])] = pc\n",
    "\n",
    "            # Recent competence (rpc)\n",
    "            rpc = x.rolling(min_periods=kwargs['window_size'], window=kwargs['window_size']).mean()\n",
    "            df.loc[(i, a), 'rpc{}'.format(a[1])] = rpc\n",
    "\n",
    "            # Recent learning progress (rlp)\n",
    "            rlp = x.rolling(min_periods=kwargs['window_size'], window=kwargs['window_size']).apply(\n",
    "                rlp_func, args=(kwargs['subwindow_size_1'], kwargs['subwindow_size_2']), raw=False\n",
    "            )\n",
    "            df.loc[(i, a), 'rlp{}'.format(a[1])] = rlp\n",
    "        \n",
    "        df.loc[(i, slice(None)), :] = df.loc[(i, slice(None)), :].fillna(method='ffill', axis=0)\n",
    "\n",
    "        # Self-challenge (sc)\n",
    "        rpc_max = df.loc[(i, slice(None)), 'rpc1':'rpc4'].max(axis=1).rolling(min_periods=1, window=250).max()\n",
    "        rpc_min = df.loc[(i, slice(None)), 'rpc1':'rpc4'].min(axis=1).rolling(min_periods=1, window=250).min()\n",
    "        act_inds = np.array([act_codes[a] for a in sdf.index.get_level_values(1).tolist()]) - 1\n",
    "        current_rpc = df.loc[(i, slice(None)), 'rpc1':'rpc4'].values[np.arange(60+250), act_inds]\n",
    "        sc = 1 - (current_rpc-rpc_min)/(rpc_max-rpc_min)\n",
    "        df.loc[(i, slice(None)), 'sc'] = sc\n",
    "\n",
    "    df = df.reset_index().sort_values(by=['sid', 'trial'])\n",
    "    df.loc[df.stage=='train', 'sc'] = np.nan    # SC is not defined in familizarization stage\n",
    "    display(df.loc[(df.sid == 0) & (df.trial >= 1) & (df.trial < 70), :])    # Display data excerpt\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "make_heuristics_dataset(\n",
    "    input_data_path = 'data/clean_data.csv',\n",
    "    save_path = 'data/heuristics_data.csv',\n",
    "    window_size = 15,\n",
    "    subwindow_size_1 = 10,\n",
    "    subwindow_size_2 = 6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NAM designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T23:16:28.101626Z",
     "start_time": "2020-11-17T23:16:27.409110Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_mps(df, **kwargs):\n",
    "    '''Find mastery points'''\n",
    "    arr = df.values\n",
    "    mask = (arr != 0)\n",
    "    arr = np.where(mask.any(axis=0), mask.argmax(axis=0), kwargs['invalid_val'])\n",
    "    return pd.Series(arr, dtype=kwargs['dtype'])\n",
    "\n",
    "\n",
    "def make_nam_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_data_path, index_col='sid')\n",
    "\n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial >= 60)]\n",
    "    df.loc[:, 'trial'] -= 60\n",
    "    \n",
    "    # Get group dataset\n",
    "    group_df = df.groupby('sid').head(1)[['group']]\n",
    "\n",
    "    # Evaluate each trial's recent PC to True if mastery criterion was reached\n",
    "    mastered = df.reset_index().set_index(['sid','trial']).loc[:, 'rpc1':'rpc3'] >= kwargs['crit']\n",
    "    \n",
    "    # For each subject, find mastery points and NAM\n",
    "    by_sid = mastered.groupby('sid')\n",
    "    mastery_points = by_sid.apply(get_mps, invalid_val=250, dtype='int')\n",
    "    mastery_points.rename(columns={0:'mp1', 1:'mp2', 2:'mp3'}, inplace=True)\n",
    "    nam = by_sid.any().sum(axis=1).to_frame(name='nam')\n",
    "\n",
    "    # Display output dataset excerpt\n",
    "    nam = group_df.merge(nam, on='sid')\n",
    "    nam_df = nam.merge(mastery_points, on='sid').reset_index()\n",
    "    display(nam_df.head(10))\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        nam_df.to_csv(save_path, index=False)    \n",
    "\n",
    "\n",
    "make_nam_dataset(\n",
    "    input_data_path = 'data/heuristics_data.csv', \n",
    "    save_path = 'data/nam_data.csv',\n",
    "    crit = 13/15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T23:18:13.320605Z",
     "start_time": "2020-11-17T23:18:04.963418Z"
    },
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_learning_dataset(heuristics_data_path, nam_data_path, save_path, **kwargs):    \n",
    "    # Load heuristics data\n",
    "    df = pd.read_csv(heuristics_data_path, index_col='sid')\n",
    "    \n",
    "    # Add NAM classification\n",
    "    df = df.merge(pd.read_csv(nam_data_path, index_col='sid').drop(columns='group'), on='sid')\n",
    "    \n",
    "    # Annotate switch trials\n",
    "    df['switch'] = 0\n",
    "    choices = df.activity.values\n",
    "    switches = df.switch.values.copy() \n",
    "    switches[1:] = choices[:-1] != choices[1:]\n",
    "    df.loc[:, 'switch'] = switches\n",
    "    df.loc[df.stage=='train', 'switch'] = 0  # changing activity during forced stage is not switching\n",
    "    df.loc[df.trial==61, 'switch'] = 0       # choosing activity for the first time is not switching\n",
    "    \n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial > 60)]\n",
    "    df.loc[:, 'trial'] -= 61\n",
    "    \n",
    "    # For each subject, compute learning stats from free play stage\n",
    "    df.reset_index(inplace=True)\n",
    "    outdf = []\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress: '):\n",
    "        _sdf = sdf.set_index('trial') # index subject data by trial\n",
    "    \n",
    "        # Get subject information (group, nam, mps) as a pandas Series\n",
    "        profile = _sdf.head(1)[['group', 'nam', 'mp1', 'mp2', 'mp3']].iloc[0]\n",
    "\n",
    "        # Get intervals between consecutive mastery points\n",
    "        mps = profile['mp1':'mp3'].values\n",
    "        sorted_lep_bounds = np.sort(np.unique([0] + mps.tolist() + [250]))  \n",
    "        lep_intervals = pd.IntervalIndex.from_arrays(sorted_lep_bounds[:-1], sorted_lep_bounds[1:], closed='right')\n",
    "\n",
    "        # Get intervals between consecutive swiches\n",
    "        switch_trials = _sdf.switch.values.nonzero()[0].tolist() + [250]\n",
    "        if switch_trials[0] != 1: switch_trials.insert(0,1)\n",
    "        streaks = pd.IntervalIndex.from_arrays(switch_trials[:-1], switch_trials[1:], closed='right')\n",
    "\n",
    "        # Calculate self-challenge (SC) summaries\n",
    "        sc = _sdf.sc\n",
    "        sc_flat = np.mean(sc)\n",
    "        sc_lep = sc.groupby(pd.cut(_sdf.index.astype(int), lep_intervals)).mean().mean()\n",
    "        sc_streaks = sc.groupby(pd.cut(_sdf.index.astype(int), streaks)).mean().mean()    \n",
    "\n",
    "        # Calculate weighted initial (dwipc) and final (dwfpc) performances\n",
    "        dwipc = (_sdf.loc[0, 'rpc1':'rpc3'].values * kwargs['difficulty_weights']).sum()\n",
    "        dwfpc = (_sdf.loc[249, 'rpc1':'rpc3'].values * kwargs['difficulty_weights']).sum()\n",
    "\n",
    "        # Get profile info and see if subject mastered activities in order of difficulty\n",
    "        sid = i\n",
    "        group = profile['group']\n",
    "        nam = profile['nam']\n",
    "        progressive = (np.diff(np.array([1,2,3])[np.argsort(mps)]) == 1).all()\n",
    "        \n",
    "        # Store subject's learning stats\n",
    "        outdf.append(\n",
    "            pd.Series(\n",
    "                data = [sid,group,nam,progressive,dwipc,dwfpc,sc_flat,sc_lep,sc_streaks], \n",
    "                index='sid,group,nam,progressive,dwipc,dwfpc,sc_flat,sc_lep,sc_streaks'.split(',')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    outdf = pd.DataFrame(outdf).sort_values(by=['group','sid'])\n",
    "    display(outdf.head())\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        outdf.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "make_learning_dataset(\n",
    "    heuristics_data_path = 'data/heuristics_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    save_path = 'data/learning_data.csv',\n",
    "    difficulty_weights = np.array([1,2,3])/6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Choice-modeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T22:50:53.634856Z",
     "start_time": "2020-11-16T22:50:44.857689Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prep_modeling_data(heuristics_data_path, nam_data_path, save_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(heuristics_data_path, index_col='sid')\n",
    "    \n",
    "    # Combine with NAM dataset\n",
    "    df = df.merge(pd.read_csv(nam_data_path, index_col='sid'), on='sid')\n",
    "    \n",
    "    # Encode activity choices as one-hot vectors\n",
    "    activity_codes = df.activity.str.get(1)\n",
    "    df = pd.concat([df, pd.get_dummies(activity_codes, prefix='ch', prefix_sep='')], axis = 1)\n",
    "    \n",
    "    add_data, act_inds = [], ['1','2','3','4']\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        # Get relative time data\n",
    "        trials_per_activity = sdf.loc[:, 'ch1':'ch4'].cumsum(axis=0)\n",
    "        trials_total = np.tile(np.arange(trials_per_activity.shape[0]) + 1, [4, 1]).T\n",
    "        relt = trials_per_activity / trials_total\n",
    "        relt.columns = ['relt' + i for i in act_inds]\n",
    "        \n",
    "        # Get previous trial choice data\n",
    "        prev = sdf.loc[:, 'ch1':'ch4']\n",
    "        prev.iloc[1:, :] = prev.iloc[:-1, :]\n",
    "        prev.iloc[0, :] = np.nan\n",
    "        prev.columns = ['prev' + i for i in act_inds]\n",
    "        \n",
    "        # Store into list\n",
    "        add_data.append([relt, prev])\n",
    "    \n",
    "    add_data = pd.concat([pd.concat(h, axis=0) for h in zip(*add_data)], axis=1)\n",
    "    df = pd.concat([df, add_data], axis=1).reset_index()\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "prep_modeling_data(\n",
    "    heuristics_data_path = 'data/heuristics_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    save_path = 'data/model_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fitted parameters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T20:57:49.548250Z",
     "start_time": "2020-11-18T20:57:48.885547Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prep_fitted_data(fitted_models_data_path, save_path=''):\n",
    "    # Load data\n",
    "    df = pd.read_csv(fitted_models_data_path).set_index(['sid', 'vars'])\n",
    "    \n",
    "    # Initialize empty dict to turn into DF\n",
    "    var_names = df.index.get_level_values(1).to_series().max().split(',')\n",
    "    col_names = ['sid', 'vars'] + var_names + ['tau']\n",
    "    df_dict = dict(zip(col_names, [[] for _ in col_names]))\n",
    "    \n",
    "    # Iterate through DF and extract model params from stored csv strings\n",
    "    for i, row in df.iterrows():\n",
    "        df_dict['sid'].append(i[0])\n",
    "        df_dict['vars'].append(i[1])\n",
    "        params = row.params.split(',')\n",
    "        df_dict['tau'].append(params.pop())\n",
    "        vars_included = i[1].split(',')\n",
    "        for vn in var_names:\n",
    "            df_dict[vn].append(params[vars_included.index(vn)] if vn in vars_included else np.nan)\n",
    "\n",
    "    # Generate DF from df_dict and merge with initial DF\n",
    "    df = df.filter(items=['group','nam','aic']).merge(\n",
    "        right = pd.DataFrame(df_dict).set_index(['sid', 'vars']),\n",
    "        on = ['sid', 'vars']\n",
    "    ).reset_index()\n",
    "    \n",
    "    display(df.head())\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "prep_fitted_data(\n",
    "    fitted_models_data_path = 'data/model_results/param_fits_raw.csv',\n",
    "    save_path = 'data/model_results/param_fits_clean.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
